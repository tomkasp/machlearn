{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://spark.apache.org/images/spark-logo.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Email leakage prevention using Spark and Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Overview\n",
    "* What is the problem and why is it important?\n",
    "* What approach have we used?\n",
    "* What was our data?\n",
    "* Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's have some algorithm in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Introduction\n",
    "It's bright new and it's sparkling!\n",
    "\n",
    "<img style=\"float: left;\" src = \"http://www.scala-lang.org/resources/img/smooth-spiral.png\" > <img style=\"float: left;\" src=\"http://insights.dice.com/wp-content/uploads/2012/04/java_logo.jpg\">   <img style=\"float: left;\" src = \"http://jeroenooms.github.io/r-dependency-versioning/slides/logo.png\" > <img style=\"float: left;\" src = \"http://www.element14.com/community/servlet/JiveServlet/downloadImage/38-13581-176681/140-140/python-logo.png\" >\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://spark.apache.org/images/logistic-regression.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression in Hadoop and Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDDs - Resilent Distributed Datasets\n",
    "A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents an immutable, partitioned collection of elements that can be operated on in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Setting up Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "master = \"local\"\n",
    "sc = SparkContext(master, \"LEAKAGE\")\n",
    "sql_ctx = SQLContext(sc)\n",
    "print('DONE! Using Spark version', sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Creating RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = ['Joe', 'Tomek', 'Kate']\n",
    "namesRDD = sc.parallelize(names)\n",
    "print(type(namesRDD), namesRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RDDcollected = namesRDD.collect()\n",
    "print(type(RDDcollected), RDDcollected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def term_frequency(some_text):\n",
    "        from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "        hashingTF = HashingTF()\n",
    "        featurizedData = hashingTF.transform(some_text.split())\n",
    "        return featurizedData.collect()[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from data_abstraction import MessageEntry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = MessageEntry(body='         Hello    World!')\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(a.__eq__(a))\n",
    "from copy import deepcopy\n",
    "b = deepcopy(a)\n",
    "b.mid = 10\n",
    "print(a.__eq__(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sql_ctx.read.json('messages.json')\n",
    "#print(type(df), df)\n",
    "data = df.rdd.map(lambda row: MessageEntry(mid = row[6], body = row[1], from_field = row[5], subject = row[8], \\\n",
    "                 date = row[3], owner = row[7], to = row[9], cc = row[2], bcc = row[0], folder=row[4]))\n",
    "#print(type(q))\n",
    "print(\"Number of entries\", data.count())\n",
    "print(data.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RDD Operations\n",
    "#### Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from numpy import all, any\n",
    "folders = ['Sent', 'All documents']\n",
    "data = data.filter(lambda message: all([folder not in message.folder for folder in folders]))\n",
    "\n",
    "print(\"Number of entries\", data.count())\n",
    "print(data.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data.filter(lambda message: message.date != \"\")\n",
    "print(\"Number of entries\", data.count())\n",
    "print(data.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data.distinct()\n",
    "print(\"Number of entries\", data.count())\n",
    "print(data.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unification_dict = {'vkaminski@aol.com':'vince.kaminski@enron.com', 'vkamins@enron.com':'vince.kaminski@enron.com', 'vince.j.kaminski@enron.com':'vince.kaminski@enron.com', 'j.kaminski@enron.com':'vince.kaminski@enron.com'}\n",
    "\n",
    "def unificate(message):\n",
    "    if message.from_field in unification_dict.keys():\n",
    "        message.from_field = unification_dict[message.from_field]\n",
    "    for idx, recipient in enumerate([message.cc, message.bcc, message.to]):\n",
    "        if recipient is not None:\n",
    "            rec_list = []\n",
    "            for address in recipient:\n",
    "                if address in unification_dict.keys():\n",
    "                    rec_list.append(unification_dict[address])\n",
    "                else:\n",
    "                    rec_list.append(address)\n",
    "            if idx == 0:\n",
    "                message.cc = tuple(rec_list)\n",
    "            elif idx == 1:\n",
    "                message.bcc = tuple(rec_list)\n",
    "            else:\n",
    "                message.to = tuple(rec_list)\n",
    "            \n",
    "    return message\n",
    " \n",
    "data = data.map(lambda message: unificate(message))\n",
    "print(\"Number of entries\", data.count())\n",
    "print(data.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "####CombineByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = data.map(lambda message: (message.owner, message))\n",
    "print(\"Number of entries\", data.count())\n",
    "print(data.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ...:         .aggregateByKey(\n",
    "# ...:                    # Value to start aggregation (passed as s to `lambda s, d`)\n",
    "# ...:                    \"start\",\n",
    "# ...:                    # Function to join final data type (string) and rdd data type\n",
    "# ...:                    lambda s, d: \"[ %s %s ]\" % (s, d[\"value\"]),\n",
    "# ...:                    # Function to join two final data types.\n",
    "# ...:                    lambda s1, s2: \"{ %s %s }\" % (s1, s2),\n",
    "# ...:                    ) \\\n",
    "\n",
    "data = data.aggregateByKey(0, lambda number, message: number+1, lambda number1, number2: number1 + number2)\n",
    "print(\"Number of entries\", data.count())\n",
    "print(data.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Explore on your own!\n",
    "https://spark.apache.org/docs/latest/programming-guide.html\n",
    "\n",
    "http://spark.apache.org/docs/latest/ml-features.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# insert code below and have fun\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
